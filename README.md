# ğŸ§  RAG-LLM: Wikipedia Q\&A System with Evaluation

## 1. ğŸš€ Project Overview

This project implements an end-to-end Retrieval-Augmented Generation (RAG) system designed to generate high-quality answers from scraped Wikipedia pages under the â€œOutline of computer scienceâ€ category. The solution leverages state-of-the-art tools and models to build a production-ready pipeline, supporting everything from data scraping to question-answer evaluation.

### ğŸŒ Scraped Pages:

* Pages under Wikipediaâ€™s "Outline of Computer Science"
* Categories: Algorithms, Data Structures, Programming Languages, Databases, Networking, AI, etc.

### ğŸ“‘ Key Features:

* **Scraping** Wikipedia pages and storing them as clean JSON
* **Chunking** the content with `RecursiveCharacterTextSplitter`
* **Embedding** using **BAAI/bge-large-en-v1.5** model
* **Storing** embeddings into **FAISS** vector database
* **Reranking** with `cross-encoder/ms-marco-MiniLM-L-6-v2`
* **Answer generation** with `llama3:8b` via **Ollama**
* **UI** built in **Streamlit**
* **Evaluation** using both LLM-based (RAGAS) and non-LLM-based (BERTScore, ROUGE, BLEU) methods

## 2. ğŸ—‚ï¸ Folder Structure

```bash
RAG Project/
â”‚
â”œâ”€â”€ app/                        # Streamlit UI
â”‚   â”œâ”€â”€ ui_app.py              # Main chatbot UI
â”‚   â”œâ”€â”€ test_chatbot_ui.py     # UI test cases
â”‚   â””â”€â”€ logs/                  # Chat interaction logs
â”‚
â”œâ”€â”€ scrapers/                  # Scraping Wikipedia content
â”‚   â”œâ”€â”€ wiki_scraper.py        # Core scraper module
â”‚   â”œâ”€â”€ test_scraper.py        # Unit tests
â”‚   â”œâ”€â”€ test_result_scraper/   # Logs and results
â”‚   â””â”€â”€ data/                  # Raw JSON output from scraping
â”‚
â”œâ”€â”€ src/                       # Core RAG pipeline
â”‚   â”œâ”€â”€ rag_pipeline.py        # Chunking + embedding + saving
â”‚   â”œâ”€â”€ mini_rag.py            # Minimal version for testing
â”‚   â”œâ”€â”€ test_ragPipeline.py    # Pipeline unit tests
â”‚   â””â”€â”€ embeddings/            # FAISS index stored here
â”‚
â”œâ”€â”€ GroundTruth_Context_Generation/
â”‚   â”œâ”€â”€ eval_data.csv               # Ground-truth Q&A
â”‚   â”œâ”€â”€ eval_data_enriched.csv     # Enriched with generated answers/context
â”‚   â”œâ”€â”€ groundTruth_context.py     # Script to enrich with generation
â”‚ 
â”‚
â”œâ”€â”€ bert_evaluation/               # Traditional metric-based evaluation
â”‚   â”œâ”€â”€ bert_score_eval.py         # Main script (BERTScore, ROUGE, BLEU)
â”‚   â”œâ”€â”€ input.csv                  # Evaluation input
â”‚   â””â”€â”€ bert_eval_output/
â”‚       â”œâ”€â”€ logs/                  # Logs
â”‚       â””â”€â”€ outputs/               # Evaluation results
â”‚
â”œâ”€â”€ ragas_eval_custom/            # Gemini/LLM-based custom evaluator
â”‚
â”œâ”€â”€ requirements.txt              # Dependencies
â””â”€â”€ README.md
```

## 3. ğŸ§­ Architecture Overview

*ğŸ“Œ Insert architecture image showing the full RAG pipeline with scraping â†’ chunking â†’ embedding â†’ retrieval â†’ reranking â†’ generation â†’ evaluation.*

---

## 4. ğŸ•¸ï¸ Scraping Pipeline

### Core Module: `wiki_scraper.py`

* Uses `wikipediaapi` and `BeautifulSoup` to extract clean text
* Targets pages from the "Outline of Computer Science"
* Saves JSON content in `scrapers/data/raw_scraped/`
* Logs every scraped URL

### Workflow:

1. Delete old scraped files to save space
2. Recursively navigate linked pages
3. Extract paragraphs and skip unrelated content
4. Save clean structured output in JSON

### Key Methods:

* `scrape_outline()`: Starts the full scraping session
* `parse_page()`: Extracts clean text from one page
* `save_json()`: Dumps structured content to disk

---

## 5. âœ‚ï¸ Chunking, Embedding & FAISS Storage

### Chunking:

* **Tool:** `RecursiveCharacterTextSplitter`
* **Config:** chunk size = 512, overlap = 50

### Comparison of Chunking Strategies:

| Method                         | Pros                                | Cons                        |
| ------------------------------ | ----------------------------------- | --------------------------- |
| Fixed-size (naÃ¯ve)             | Simple, fast                        | Breaks context mid-sentence |
| RecursiveCharacterTextSplitter | Preserves structure (headings etc.) | Slightly slower             |
| Token-based (e.g., tiktoken)   | More accurate with LLMs             | More complex                |

â¡ï¸ **Selected RecursiveCharacterTextSplitter** to ensure sentence boundaries and heading preservation.

### Embedding:

* **Model Used:** `BAAI/bge-large-en-v1.5`
* **Why Chosen:**

  * Optimized for English
  * Trained for retrieval tasks
  * Better semantic separation

### Model Specifications:

* Parameters: 300M+
* Dimensionality: 1024
* Pretrained on English web/corpora

### Comparison of Embedding Models:

| Model                                    | Dim  | Retrieval Quality | Speed  | Comments                      |
| ---------------------------------------- | ---- | ----------------- | ------ | ----------------------------- |
| `sentence-transformers/all-MiniLM-L6-v2` | 384  | Medium            | Fast   | Lightweight, baseline         |
| `BAAI/bge-small-en-v1.5`                 | 384  | Medium-High       | Fast   | Good for compact systems      |
| `BAAI/bge-large-en-v1.5`                 | 1024 | **High**          | Medium | Best semantic matching so far |
| `multi-qa-mpnet-base-dot-v1`             | 768  | High              | Medium | Multi-lingual support         |

### Workflow:

1. Load JSON data
2. Clean and deduplicate chunks
3. Apply recursive chunking
4. Generate embeddings via `bge-large-en-v1.5`
5. Save to FAISS index in `/src/embeddings/full_faiss_index/`

---

## 6. ğŸ§‘â€ğŸ’» Streamlit UI Chatbot

### File: `app/ui_app.py`

* Loads FAISS index
* Retrieves top-k chunks
* Reranks using `cross-encoder/ms-marco-MiniLM-L-6-v2`
* Sends best-ranked chunks to LLaMA 3 (8B) via Ollama
* Displays chat interface and logs interactions

### UI Features:

* Clear historical context display
* Real-time generation
* Logging user queries and responses

---

## 7. ğŸ“Š Evaluation Strategy

### A. BERT-based Traditional Evaluation

#### File: `bert_score_eval.py`

* Input: `input.csv` with Q, Ground Truth A, Generated A
* Output: ROUGE, BLEU, BERTScore

#### Metrics Used:

| Metric        | Compares                     | Score Range | Meaning                                   |
| ------------- | ---------------------------- | ----------- | ----------------------------------------- |
| **BLEU**      | n-gram overlap               | 0â€“1         | Precision-oriented score                  |
| **ROUGE**     | recall-oriented overlap      | 0â€“1         | Measures recall of n-grams                |
| **BERTScore** | contextual embeddings (BERT) | 0â€“1         | Semantic similarity (better than n-grams) |

* **BLEU**: Measures how much the generated answer overlaps with ground truth using n-gram matching.
* **ROUGE**: Measures how much of the ground truth is captured in the generated answer.
* **BERTScore**: Uses BERT embeddings to compare semantic similarity of sentences.

### B. LLM-Based (RAGAS-style)

* File: `ragas_eval_custom/`
* Uses Gemini to score:

  * Faithfulness
  * Context relevance
  * Answer correctness
  * Factual accuracy

### Comparison:

| Metric Type | Tool Used | Key Insight                 |
| ----------- | --------- | --------------------------- |
| Non-LLM     | BERTScore | Precise but surface level   |
| LLM-based   | Gemini    | Deeper, contextual judgment |

---

## 8. ğŸ› ï¸ Improvements and Enhancements

* Rewrote scraper to log and clean more effectively
* Replaced `bge-small` with **bge-large-en-v1.5** for better semantic quality
* Integrated reranker to boost answer relevance
* Improved chunking strategy (overlap + recursion)
* Streamlit UI now shows previous questions and logs interactions
* Enhanced evaluation: both LLM and traditional metrics now included

---

---
## 9. â¡ï¸ How to Run

* # Install dependencies
* pip install -r requirements.txt

* # Scrape Wikipedia
* python scrapers/wiki_scraper.py

* # Run chunking and embedding
* python src/rag_pipeline.py

* # Launch chatbot UI
* streamlit run app/ui_app.py

* # Evaluate with BERT
* python bert_evaluation/bert_score_eval.py

---

## âœ… Future Work

* Complete RAGA based testing
* Add support for multilingual scraping and embedding
* Experiment with hybrid reranking (BM25 + cross-encoder)
* Extend evaluation framework with Hallucination detection (e.g., TruthfulQA)
* Integrate feedback loop to improve QA generation over time

---

## ğŸ“Œ License

MIT License

## ğŸ™Œ Acknowledgements

* BAAI for BGE model
* HuggingFace for Transformers & evaluation
* Langchain for inspiration on RAG design
* Google Gemini, and Meta for LLM APIs
